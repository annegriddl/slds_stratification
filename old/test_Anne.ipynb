{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test to save all estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'pexpect'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "import json\n",
    "import os\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from utils import FriedmanDataset, ModelOptimizerFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n_train': 200, 'noise': 0, 'transformation': 'identity', 'group_size': 5},\n",
       " {'n_train': 200, 'noise': 0, 'transformation': 'identity', 'group_size': 10},\n",
       " {'n_train': 200, 'noise': 0, 'transformation': 'sqrt', 'group_size': 5},\n",
       " {'n_train': 200, 'noise': 0, 'transformation': 'sqrt', 'group_size': 10},\n",
       " {'n_train': 200, 'noise': 5, 'transformation': 'identity', 'group_size': 5},\n",
       " {'n_train': 200, 'noise': 5, 'transformation': 'identity', 'group_size': 10},\n",
       " {'n_train': 200, 'noise': 5, 'transformation': 'sqrt', 'group_size': 5},\n",
       " {'n_train': 200, 'noise': 5, 'transformation': 'sqrt', 'group_size': 10},\n",
       " {'n_train': 1000, 'noise': 0, 'transformation': 'identity', 'group_size': 5},\n",
       " {'n_train': 1000, 'noise': 0, 'transformation': 'identity', 'group_size': 10},\n",
       " {'n_train': 1000, 'noise': 0, 'transformation': 'sqrt', 'group_size': 5},\n",
       " {'n_train': 1000, 'noise': 0, 'transformation': 'sqrt', 'group_size': 10},\n",
       " {'n_train': 1000, 'noise': 5, 'transformation': 'identity', 'group_size': 5},\n",
       " {'n_train': 1000, 'noise': 5, 'transformation': 'identity', 'group_size': 10},\n",
       " {'n_train': 1000, 'noise': 5, 'transformation': 'sqrt', 'group_size': 5},\n",
       " {'n_train': 1000, 'noise': 5, 'transformation': 'sqrt', 'group_size': 10}]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "json_file = \"./test.json\" # set path to save json-file \n",
    "# Data Creation\n",
    "n_test= 100000\n",
    "n_features = 10\n",
    "# Random Search Parameters\n",
    "n_folds = 5\n",
    "n_iter= 20\n",
    "scoring= 'neg_mean_squared_error' #Metriken anschauen\n",
    "n_jobs= -1\n",
    "# Anzahl Wiederholungen\n",
    "n_repetitions = 10\n",
    "\n",
    "\n",
    "\n",
    "# Define hyperparameter options\n",
    "train_list = [200, 1000]\n",
    "noise_list = [0, 5]\n",
    "transformation_list = ['identity', 'sqrt']\n",
    "group_size_list = [5, 10]\n",
    "\n",
    "\n",
    "all_combinations = generate_hyperparameter_combinations_dict(n_train=train_list, \n",
    "                                                             noise=noise_list,\n",
    "                                                             transformation=transformation_list, \n",
    "                                                             group_size=group_size_list\n",
    "                                                             )\n",
    "all_combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 200\n",
      "1 200\n",
      "1 200\n",
      "1 200\n",
      "1 200\n",
      "1 200\n",
      "1 200\n",
      "1 200\n",
      "1 1000\n",
      "1 1000\n",
      "1 1000\n",
      "1 1000\n",
      "1 1000\n",
      "1 1000\n",
      "1 1000\n",
      "1 1000\n"
     ]
    }
   ],
   "source": [
    "# Display the combined hyperparameter dictionaries\n",
    "for hyperparameter_dict in all_combinations:\n",
    "    print(\"1\", hyperparameter_dict['n_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_hyperparameter_combinations_dict(**hyperparameters):\n",
    "    \"\"\"\n",
    "    Generate all possible combinations of hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    **hyperparameters: Variable keyword arguments where the key is the hyperparameter name and the value is an iterable of options (best: list).\n",
    "\n",
    "    Returns:\n",
    "    List of dictionaries, where each dictionary represents a combination of hyperparameters.\n",
    "    \"\"\"\n",
    "    hyperparameter_names = hyperparameters.keys()\n",
    "    all_hyperparameter_combinations = list(itertools.product(*hyperparameters.values()))\n",
    "\n",
    "    all_hyperparameter_dicts = []\n",
    "    for combination in all_hyperparameter_combinations:\n",
    "        hyperparameter_dict = dict(zip(hyperparameter_names, combination))\n",
    "        all_hyperparameter_dicts.append(hyperparameter_dict)\n",
    "\n",
    "    return all_hyperparameter_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = [200, 1000]\n",
    "n_features = 10\n",
    "FD_noise = [0, 5]\n",
    "transformation = ['identity', 'sqrt']\n",
    "group_size = [5, 10]\n",
    "n_folds = 5\n",
    "n_iter = 200\n",
    "itertools.combinations(n_train, FD_noise, transformation, group_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'RandomForestRegressor',\n",
       " 'n_train': 200,\n",
       " 'n_test': 100000,\n",
       " 'n_features': 10,\n",
       " 'FD_noise': 0,\n",
       " 'transformation': 'sqrt',\n",
       " 'n_groups': 10,\n",
       " 'n_folds': 5,\n",
       " 'n_iter': 20,\n",
       " 'n_repetitions': 10,\n",
       " 'scoring': 'neg_mean_squared_error',\n",
       " 'n_jobs': -1,\n",
       " 'json_file': './test.json'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file = \"./test.json\" # set path to save json-file \n",
    "# Data Creation\n",
    "n_train= 200\n",
    "n_test= 100000\n",
    "n_features = 10\n",
    "FD_noise= 0\n",
    "transformation='sqrt'\n",
    "seed=567\n",
    "\n",
    "# Stratification\n",
    "group_size = 10\n",
    "\n",
    "# RandomSearchCV\n",
    "n_folds = 5\n",
    "n_iter= 20\n",
    "n_repetitions= 10\n",
    "scoring= 'neg_mean_squared_error' #Metriken anschauen\n",
    "n_jobs= -1\n",
    "\n",
    "RF_param_grid = {\n",
    "    'min_samples_split': np.arange(2, 21),\n",
    "    'min_samples_leaf': np.arange(1, 21),\n",
    "    'max_features': np.arange(1, n_features + 1) #@nadja is that right?\n",
    "}\n",
    "\n",
    "# Initalize Model\n",
    "modelOptimizer = ModelOptimizerFinal(RandomForestRegressor(random_state=seed), \n",
    "                                RF_param_grid,\n",
    "                                random_state=seed)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=1000, random_state=seed) \n",
    "# Save Parameters in a dictionary\n",
    "params = {'model': modelOptimizer.model.__class__.__name__,\n",
    "          'n_train': n_train,\n",
    "          'n_test': n_test,\n",
    "          'n_features': n_features,\n",
    "          'FD_noise': FD_noise,\n",
    "          'transformation': transformation,\n",
    "          'n_groups': group_size,\n",
    "          'n_folds': n_folds,\n",
    "          'n_iter': n_iter,\n",
    "          'n_repetitions': n_repetitions,\n",
    "          'scoring': scoring, \n",
    "          'n_jobs': n_jobs,\n",
    "          'json_file': json_file}\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = generate_friedman1(n_samples=n_test,\n",
    "                                                     n_features=n_features,\n",
    "                                                     noise=FD_noise,\n",
    "                                                     random_state=1718)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_friedman1(n_samples=n_train,\n",
    "                                                        n_features=n_features,\n",
    "                                                        noise=FD_noise,\n",
    "                                                        random_state=random_state,\n",
    "                                                        transformation=transformation)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the data\n",
    "if np.isnan(y_train).any() or np.isnan(y_test).any(): \n",
    "                X_train, y_train = make_friedman1(n_samples=n_train,\n",
    "                                    n_features=n_features, \n",
    "                                    noise=noise, \n",
    "                                    random_state=random_states[repetition])\n",
    "                X_test, y_test = make_friedman1(n_samples=n_test,\n",
    "                                    n_features=n_features,\n",
    "                                    noise=noise,\n",
    "                                    random_state=1718)\n",
    "                min_val = min(y_train.min(), y_test.min())\n",
    "                # @Anne: Noch absprechen... +1 weil es sonst Skala durch Werte [0, 1] hauptsächlich bei y_test größer wird.           \n",
    "                y_train = y_train + abs(min_val) + 1\n",
    "                y_test = y_test + abs(min_val) + 1\n",
    "                if transformation=='identity':\n",
    "                    pass\n",
    "                elif transformation == 'log':\n",
    "                    y_train = np.log(y_train)\n",
    "                    y_test = np.log(y_test)\n",
    "                elif transformation == 'sqrt':\n",
    "                    y_train = np.sqrt(y_train)\n",
    "                    y_test = np.sqrt(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "cv_splits = n_folds\n",
    "random_state = 45\n",
    "start_time = time.time()\n",
    "random_search = RandomizedSearchCV(estimator=model,\n",
    "                                           param_distributions=RF_param_grid,\n",
    "                                           n_iter=n_iter,\n",
    "                                           cv=cv_splits,\n",
    "                                           scoring=scoring,\n",
    "                                           n_jobs=n_jobs,\n",
    "                                           random_state=random_state,\n",
    "                                           verbose=10)\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.47038507461548"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_time = end_time - start_time\n",
    "running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unstratified = random_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([5.00990644, 5.65276647, 5.11320963, 5.92497511, 5.98721724,\n",
       "        4.36695023, 4.61709428, 3.67889953, 4.29646788, 3.97168398,\n",
       "        5.54078302, 4.33252978, 4.33474655, 4.59259844, 4.80369287,\n",
       "        4.32539926, 6.72153368, 5.40253696, 6.19697189, 4.97057409]),\n",
       " 'std_fit_time': array([0.17157758, 0.27067507, 0.12507472, 0.12701229, 0.14979604,\n",
       "        0.191261  , 0.24553393, 0.24391681, 0.10954427, 0.09218986,\n",
       "        0.18261605, 0.13713153, 0.22126044, 0.26291373, 0.25915343,\n",
       "        0.1929958 , 0.24840166, 0.29750458, 0.14446448, 0.56954533]),\n",
       " 'mean_score_time': array([0.14109292, 0.15246305, 0.14343815, 0.12090912, 0.13220224,\n",
       "        0.12727895, 0.12575464, 0.14821725, 0.14123712, 0.12665305,\n",
       "        0.13337579, 0.13372936, 0.14349365, 0.13539653, 0.13778725,\n",
       "        0.1336915 , 0.13298821, 0.14719005, 0.11945004, 0.1032022 ]),\n",
       " 'std_score_time': array([0.02394724, 0.00612765, 0.01504996, 0.01544715, 0.01415603,\n",
       "        0.01448361, 0.01802225, 0.0192571 , 0.0135634 , 0.01852198,\n",
       "        0.00989826, 0.01520088, 0.01405751, 0.02996355, 0.0087846 ,\n",
       "        0.00971953, 0.01423192, 0.02137873, 0.01042469, 0.01519584]),\n",
       " 'param_min_samples_split': masked_array(data=[19, 13, 16, 18, 8, 20, 18, 13, 12, 18, 20, 17, 9, 20,\n",
       "                    5, 20, 15, 2, 15, 6],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[19, 10, 8, 1, 11, 12, 20, 9, 11, 20, 15, 1, 11, 1, 11,\n",
       "                    10, 6, 14, 2, 12],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[8, 7, 5, 6, 10, 2, 10, 1, 2, 1, 10, 1, 2, 2, 4, 2, 10,\n",
       "                    9, 7, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'min_samples_split': 19,\n",
       "   'min_samples_leaf': 19,\n",
       "   'max_features': 8},\n",
       "  {'min_samples_split': 13, 'min_samples_leaf': 10, 'max_features': 7},\n",
       "  {'min_samples_split': 16, 'min_samples_leaf': 8, 'max_features': 5},\n",
       "  {'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 6},\n",
       "  {'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 10},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 12, 'max_features': 2},\n",
       "  {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 10},\n",
       "  {'min_samples_split': 13, 'min_samples_leaf': 9, 'max_features': 1},\n",
       "  {'min_samples_split': 12, 'min_samples_leaf': 11, 'max_features': 2},\n",
       "  {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 1},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 10},\n",
       "  {'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 1},\n",
       "  {'min_samples_split': 9, 'min_samples_leaf': 11, 'max_features': 2},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 2},\n",
       "  {'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 4},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 2},\n",
       "  {'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 10},\n",
       "  {'min_samples_split': 2, 'min_samples_leaf': 14, 'max_features': 9},\n",
       "  {'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 7},\n",
       "  {'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': 9}],\n",
       " 'split0_test_score': array([-0.32043318, -0.24364773, -0.23674559, -0.21373533, -0.2446695 ,\n",
       "        -0.36037161, -0.33150641, -0.42140799, -0.35158419, -0.50046156,\n",
       "        -0.28013126, -0.38022077, -0.35158419, -0.31082971, -0.28083208,\n",
       "        -0.3433318 , -0.1986601 , -0.27340208, -0.19497254, -0.25583131]),\n",
       " 'split1_test_score': array([-0.20593885, -0.14440722, -0.14087768, -0.134997  , -0.15645816,\n",
       "        -0.23652483, -0.21810561, -0.28608999, -0.22954389, -0.34489447,\n",
       "        -0.18215646, -0.26655712, -0.22954389, -0.20671221, -0.16795847,\n",
       "        -0.22356913, -0.1298804 , -0.17490763, -0.12650074, -0.16229698]),\n",
       " 'split2_test_score': array([-0.16451662, -0.14047911, -0.13660327, -0.13188607, -0.14115104,\n",
       "        -0.17990556, -0.17553402, -0.20950973, -0.17701256, -0.23499364,\n",
       "        -0.14950374, -0.1942049 , -0.17701256, -0.16315081, -0.14725789,\n",
       "        -0.17535557, -0.13510343, -0.14397533, -0.12820222, -0.14139629]),\n",
       " 'split3_test_score': array([-0.19294715, -0.15005633, -0.14127941, -0.13292737, -0.16038716,\n",
       "        -0.20992173, -0.20199046, -0.23832683, -0.20484811, -0.2873975 ,\n",
       "        -0.17875334, -0.21742187, -0.20484811, -0.17819633, -0.16682579,\n",
       "        -0.19945898, -0.12951089, -0.17208736, -0.12525807, -0.16376885]),\n",
       " 'split4_test_score': array([-0.17062485, -0.12773517, -0.13315872, -0.11464091, -0.12570583,\n",
       "        -0.22604622, -0.17070697, -0.27207719, -0.21884745, -0.32623081,\n",
       "        -0.14280844, -0.2380565 , -0.21884745, -0.18692717, -0.15773526,\n",
       "        -0.21257627, -0.10822361, -0.14153227, -0.10491379, -0.13273586]),\n",
       " 'mean_test_score': array([-0.21089213, -0.16126511, -0.15773293, -0.14563734, -0.16567434,\n",
       "        -0.24255399, -0.21956869, -0.28548235, -0.23636724, -0.3387956 ,\n",
       "        -0.18667065, -0.25929223, -0.23636724, -0.20916324, -0.1841219 ,\n",
       "        -0.23085835, -0.14027569, -0.18118093, -0.13596947, -0.17120586]),\n",
       " 'std_test_score': array([0.05677665, 0.04184026, 0.03961861, 0.03481945, 0.04135699,\n",
       "        0.0619347 , 0.05859643, 0.07300867, 0.06024098, 0.08916004,\n",
       "        0.04924067, 0.06498759, 0.06024098, 0.05274963, 0.04892541,\n",
       "        0.05848782, 0.03061624, 0.0481312 , 0.03069377, 0.0439652 ]),\n",
       " 'rank_test_score': array([12,  5,  4,  3,  6, 17, 13, 19, 15, 20, 10, 18, 15, 11,  9, 14,  2,\n",
       "         8,  1,  7])}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_unstratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(data=[19, 13, 16, 18, 8, 20, 18, 13, 12, 18, 20, 17, 9, 20,\n",
       "                   5, 20, 15, 2, 15, 6],\n",
       "             mask=[False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False, False, False, False, False,\n",
       "                   False, False, False, False],\n",
       "       fill_value='?',\n",
       "            dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_unstratified['param_min_samples_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 479,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 483,\n",
       " 484,\n",
       " 485,\n",
       " 486,\n",
       " 487,\n",
       " 488,\n",
       " 489,\n",
       " 490,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 495,\n",
       " 496,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 543,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 547,\n",
       " 548,\n",
       " 549,\n",
       " 550,\n",
       " 551,\n",
       " 552,\n",
       " 553,\n",
       " 554,\n",
       " 555,\n",
       " 556,\n",
       " 557,\n",
       " 558,\n",
       " 559,\n",
       " 560,\n",
       " 561,\n",
       " 562,\n",
       " 563,\n",
       " 564,\n",
       " 565,\n",
       " 566,\n",
       " 567,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 571,\n",
       " 572,\n",
       " 573,\n",
       " 574,\n",
       " 575,\n",
       " 576,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 580,\n",
       " 581,\n",
       " 582,\n",
       " 583,\n",
       " 584,\n",
       " 585,\n",
       " 586,\n",
       " 587,\n",
       " 588,\n",
       " 589,\n",
       " 590,\n",
       " 591,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 596,\n",
       " 597,\n",
       " 598,\n",
       " 599,\n",
       " 600,\n",
       " 601,\n",
       " 602,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 606,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 610,\n",
       " 611,\n",
       " 612,\n",
       " 613,\n",
       " 614,\n",
       " 615,\n",
       " 616,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 621,\n",
       " 622,\n",
       " 623,\n",
       " 624,\n",
       " 625,\n",
       " 626,\n",
       " 627,\n",
       " 628,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 633,\n",
       " 634,\n",
       " 635,\n",
       " 636,\n",
       " 637,\n",
       " 638,\n",
       " 639,\n",
       " 640,\n",
       " 641,\n",
       " 642,\n",
       " 643,\n",
       " 644,\n",
       " 645,\n",
       " 646,\n",
       " 647,\n",
       " 648,\n",
       " 649,\n",
       " 650,\n",
       " 651,\n",
       " 652,\n",
       " 653,\n",
       " 654,\n",
       " 655,\n",
       " 656,\n",
       " 657,\n",
       " 658,\n",
       " 659,\n",
       " 660,\n",
       " 661,\n",
       " 662,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 669,\n",
       " 670,\n",
       " 671,\n",
       " 672,\n",
       " 673,\n",
       " 674,\n",
       " 675,\n",
       " 676,\n",
       " 677,\n",
       " 678,\n",
       " 679,\n",
       " 680,\n",
       " 681,\n",
       " 682,\n",
       " 683,\n",
       " 684,\n",
       " 685,\n",
       " 686,\n",
       " 687,\n",
       " 688,\n",
       " 689,\n",
       " 690,\n",
       " 691,\n",
       " 692,\n",
       " 693,\n",
       " 694,\n",
       " 695,\n",
       " 696,\n",
       " 697,\n",
       " 698,\n",
       " 699,\n",
       " 700,\n",
       " 701,\n",
       " 702,\n",
       " 703,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 708,\n",
       " 709,\n",
       " 710,\n",
       " 711,\n",
       " 712,\n",
       " 713,\n",
       " 714,\n",
       " 715,\n",
       " 716,\n",
       " 717,\n",
       " 718,\n",
       " 719,\n",
       " 720,\n",
       " 721,\n",
       " 722,\n",
       " 723,\n",
       " 724,\n",
       " 725,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 732,\n",
       " 733,\n",
       " 734,\n",
       " 735,\n",
       " 736,\n",
       " 737,\n",
       " 738,\n",
       " 739,\n",
       " 740,\n",
       " 741,\n",
       " 742,\n",
       " 743,\n",
       " 744,\n",
       " 745,\n",
       " 746,\n",
       " 747,\n",
       " 748,\n",
       " 749,\n",
       " 750,\n",
       " 751,\n",
       " 752,\n",
       " 753,\n",
       " 754,\n",
       " 755,\n",
       " 756,\n",
       " 757,\n",
       " 758,\n",
       " 759,\n",
       " 760,\n",
       " 761,\n",
       " 762,\n",
       " 763,\n",
       " 764,\n",
       " 765,\n",
       " 766,\n",
       " 767,\n",
       " 768,\n",
       " 769,\n",
       " 770,\n",
       " 771,\n",
       " 772,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 780,\n",
       " 781,\n",
       " 782,\n",
       " 783,\n",
       " 784,\n",
       " 785,\n",
       " 786,\n",
       " 787,\n",
       " 788,\n",
       " 789,\n",
       " 790,\n",
       " 791,\n",
       " 792,\n",
       " 793,\n",
       " 794,\n",
       " 795,\n",
       " 796,\n",
       " 797,\n",
       " 798,\n",
       " 799,\n",
       " 800,\n",
       " 801,\n",
       " 802,\n",
       " 803,\n",
       " 804,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 808,\n",
       " 809,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 816,\n",
       " 817,\n",
       " 818,\n",
       " 819,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 824,\n",
       " 825,\n",
       " 826,\n",
       " 827,\n",
       " 828,\n",
       " 829,\n",
       " 830,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 838,\n",
       " 839,\n",
       " 840,\n",
       " 841,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 845,\n",
       " 846,\n",
       " 847,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 851,\n",
       " 852,\n",
       " 853,\n",
       " 854,\n",
       " 855,\n",
       " 856,\n",
       " 857,\n",
       " 858,\n",
       " 859,\n",
       " 860,\n",
       " 861,\n",
       " 862,\n",
       " 863,\n",
       " 864,\n",
       " 865,\n",
       " 866,\n",
       " 867,\n",
       " 868,\n",
       " 869,\n",
       " 870,\n",
       " 871,\n",
       " 872,\n",
       " 873,\n",
       " 874,\n",
       " 875,\n",
       " 876,\n",
       " 877,\n",
       " 878,\n",
       " 879,\n",
       " 880,\n",
       " 881,\n",
       " 882,\n",
       " 883,\n",
       " 884,\n",
       " 885,\n",
       " 886,\n",
       " 887,\n",
       " 888,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 894,\n",
       " 895,\n",
       " 896,\n",
       " 897,\n",
       " 898,\n",
       " 899,\n",
       " 900,\n",
       " 901,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 912,\n",
       " 913,\n",
       " 914,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 920,\n",
       " 921,\n",
       " 922,\n",
       " 923,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 927,\n",
       " 928,\n",
       " 929,\n",
       " 930,\n",
       " 931,\n",
       " 932,\n",
       " 933,\n",
       " 934,\n",
       " 935,\n",
       " 936,\n",
       " 937,\n",
       " 938,\n",
       " 939,\n",
       " 940,\n",
       " 941,\n",
       " 942,\n",
       " 943,\n",
       " 944,\n",
       " 945,\n",
       " 946,\n",
       " 947,\n",
       " 948,\n",
       " 949,\n",
       " 950,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 955,\n",
       " 956,\n",
       " 957,\n",
       " 958,\n",
       " 959,\n",
       " 960,\n",
       " 961,\n",
       " 962,\n",
       " 963,\n",
       " 964,\n",
       " 965,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 972,\n",
       " 973,\n",
       " 974,\n",
       " 975,\n",
       " 976,\n",
       " 977,\n",
       " 978,\n",
       " 979,\n",
       " 980,\n",
       " 981,\n",
       " 982,\n",
       " 983,\n",
       " 984,\n",
       " 985,\n",
       " 986,\n",
       " 987,\n",
       " 988,\n",
       " 989,\n",
       " 990,\n",
       " 991,\n",
       " 992,\n",
       " 993,\n",
       " 994,\n",
       " 995,\n",
       " 996,\n",
       " 997,\n",
       " 998,\n",
       " 999,\n",
       " ...]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./seeds_available.json\", \"r\") as f:\n",
    "    seeds_available = json.load(f)\n",
    "\n",
    "seeds_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the original file\n",
    "path_to_seeds_original = './seeds_available.json'\n",
    "\n",
    "# Read the content of the original JSON file\n",
    "with open(path_to_seeds_original, 'r') as file:\n",
    "    try:\n",
    "        seeds_available = json.load(file)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error decoding JSON. The file might be empty or not properly formatted.\")\n",
    "        seeds_available = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File successfully rewritten at: ./seeds_available_fixed.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the new file\n",
    "path_to_seeds_new = './seeds_available_fixed.json'\n",
    "\n",
    "# Write the content to the new file in the correct format\n",
    "with open(path_to_seeds_new, 'w') as file:\n",
    "    json.dump(seeds_available, file, indent=4)\n",
    "\n",
    "print(f\"File successfully rewritten at: {path_to_seeds_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration - Mean Squared Error: 0.1820069225212941, Parameters: {'min_samples_split': 19, 'min_samples_leaf': 19, 'max_features': 8}\n",
      "Iteration - Mean Squared Error: 0.14134281792383813, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 10, 'max_features': 7}\n",
      "Iteration - Mean Squared Error: 0.13979710330524364, Parameters: {'min_samples_split': 16, 'min_samples_leaf': 8, 'max_features': 5}\n",
      "Iteration - Mean Squared Error: 0.12663663941987063, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 6}\n",
      "Iteration - Mean Squared Error: 0.14083267857546644, Parameters: {'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 10}\n",
      "Iteration - Mean Squared Error: 0.235536751927918, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 12, 'max_features': 2}\n",
      "Iteration - Mean Squared Error: 0.18506289664078565, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 10}\n",
      "Iteration - Mean Squared Error: 0.28783470953072376, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 9, 'max_features': 1}\n",
      "Iteration - Mean Squared Error: 0.22943321027885152, Parameters: {'min_samples_split': 12, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration - Mean Squared Error: 0.3399945854302898, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 1}\n",
      "Iteration - Mean Squared Error: 0.162355233728231, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 10}\n",
      "Iteration - Mean Squared Error: 0.2525178393037241, Parameters: {'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 1}\n",
      "Iteration - Mean Squared Error: 0.22943321027885152, Parameters: {'min_samples_split': 9, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration - Mean Squared Error: 0.1959589615824866, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 2}\n",
      "Iteration - Mean Squared Error: 0.16806468951659584, Parameters: {'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 4}\n",
      "Iteration - Mean Squared Error: 0.222631421897699, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 2}\n",
      "Iteration - Mean Squared Error: 0.11868229342172378, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 10}\n",
      "Iteration - Mean Squared Error: 0.15759405339245372, Parameters: {'min_samples_split': 2, 'min_samples_leaf': 14, 'max_features': 9}\n",
      "Iteration - Mean Squared Error: 0.11787719412389495, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 7}\n",
      "Iteration - Mean Squared Error: 0.14708141367134187, Parameters: {'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': 9}\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "results_unstratified = random_search.cv_results_\n",
    "mse_list_unstratified = []\n",
    "\n",
    "# Iterate through the results and make predictions for each iteration\n",
    "for index, (mean_score, params) in enumerate(zip(results_unstratified[\"mean_test_score\"], results_unstratified[\"params\"])):\n",
    "\n",
    "    best_model.set_params(**params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_list_unstratified.append(mse)\n",
    "\n",
    "    print(f\"Iteration {index} - Mean Squared Error: {mse}, Parameters: {params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17, 18,\n",
       "                         19, 20, 21, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, ...],\n",
       "                        array([ 10,  12,  16,  22,  23,  24,  32,  41,  45,  49,  50,  51,  66,\n",
       "        67,  73,  74,  83,  85,  90,  91, 101, 102, 107, 110, 111, 124,\n",
       "       129, 135, 137, 141, 143, 144, 155, 159, 165, 173, 177, 178, 189,\n",
       "       197], dtype=int64)),\n",
       "                       ([2, 3, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20,\n",
       "                         22, 23, 24, 27, 29, 30, 31, 32, 34, 35, 36, 37,...\n",
       "                   estimator=RandomForestRegressor(n_estimators=1000,\n",
       "                                                   random_state=567),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_features&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20])},\n",
       "                   random_state=45, scoring=&#x27;neg_mean_squared_error&#x27;,\n",
       "                   verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17, 18,\n",
       "                         19, 20, 21, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, ...],\n",
       "                        array([ 10,  12,  16,  22,  23,  24,  32,  41,  45,  49,  50,  51,  66,\n",
       "        67,  73,  74,  83,  85,  90,  91, 101, 102, 107, 110, 111, 124,\n",
       "       129, 135, 137, 141, 143, 144, 155, 159, 165, 173, 177, 178, 189,\n",
       "       197], dtype=int64)),\n",
       "                       ([2, 3, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20,\n",
       "                         22, 23, 24, 27, 29, 30, 31, 32, 34, 35, 36, 37,...\n",
       "                   estimator=RandomForestRegressor(n_estimators=1000,\n",
       "                                                   random_state=567),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={&#x27;max_features&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20]),\n",
       "                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20])},\n",
       "                   random_state=45, scoring=&#x27;neg_mean_squared_error&#x27;,\n",
       "                   verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=1000, random_state=567)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=1000, random_state=567)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=[([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 17, 18,\n",
       "                         19, 20, 21, 25, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, ...],\n",
       "                        array([ 10,  12,  16,  22,  23,  24,  32,  41,  45,  49,  50,  51,  66,\n",
       "        67,  73,  74,  83,  85,  90,  91, 101, 102, 107, 110, 111, 124,\n",
       "       129, 135, 137, 141, 143, 144, 155, 159, 165, 173, 177, 178, 189,\n",
       "       197], dtype=int64)),\n",
       "                       ([2, 3, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 19, 20,\n",
       "                         22, 23, 24, 27, 29, 30, 31, 32, 34, 35, 36, 37,...\n",
       "                   estimator=RandomForestRegressor(n_estimators=1000,\n",
       "                                                   random_state=567),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'max_features': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       "                                        'min_samples_leaf': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20]),\n",
       "                                        'min_samples_split': array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20])},\n",
       "                   random_state=45, scoring='neg_mean_squared_error',\n",
       "                   verbose=10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_splits = create_cont_folds(y=y_train, n_folds=n_folds, n_groups=20, seed=random_state)\n",
    "random_state = 45\n",
    "random_search = RandomizedSearchCV(estimator=model,\n",
    "                                           param_distributions=RF_param_grid,\n",
    "                                           n_iter=n_iter,\n",
    "                                           cv=cv_splits,\n",
    "                                           scoring=scoring,\n",
    "                                           n_jobs=n_jobs,\n",
    "                                           random_state=random_state,\n",
    "                                           verbose=10)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.80345759, 4.34663281, 3.98740835, 4.64461303, 5.00456128,\n",
       "        3.86503544, 4.68733215, 3.56064806, 3.50343966, 3.39808583,\n",
       "        4.58296652, 3.47389059, 3.48993802, 3.7865128 , 3.84228473,\n",
       "        3.46228442, 5.5025279 , 4.44740472, 4.99406586, 4.17634463]),\n",
       " 'std_fit_time': array([0.11953779, 0.18809168, 0.15560952, 0.1235797 , 0.20425711,\n",
       "        0.41863452, 0.31302975, 0.05441163, 0.06308479, 0.07235807,\n",
       "        0.07995153, 0.02154357, 0.05660094, 0.07495044, 0.0929972 ,\n",
       "        0.05607092, 0.14094468, 0.14945619, 0.13739465, 0.23138328]),\n",
       " 'mean_score_time': array([0.10980549, 0.11022758, 0.11578217, 0.11509714, 0.13213353,\n",
       "        0.12587032, 0.10664091, 0.11917171, 0.11752744, 0.11994386,\n",
       "        0.11132212, 0.12651081, 0.12832785, 0.11984725, 0.10637932,\n",
       "        0.11040831, 0.11430936, 0.1167923 , 0.11419854, 0.08524981]),\n",
       " 'std_score_time': array([0.01022314, 0.00858472, 0.01319274, 0.00534519, 0.01906321,\n",
       "        0.01495005, 0.01504643, 0.00902282, 0.0113641 , 0.02095025,\n",
       "        0.00833681, 0.01235589, 0.01820725, 0.01312504, 0.00213118,\n",
       "        0.00701381, 0.02164322, 0.0086199 , 0.00824407, 0.02220588]),\n",
       " 'param_min_samples_split': masked_array(data=[19, 13, 16, 18, 8, 20, 18, 13, 12, 18, 20, 17, 9, 20,\n",
       "                    5, 20, 15, 2, 15, 6],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[19, 10, 8, 1, 11, 12, 20, 9, 11, 20, 15, 1, 11, 1, 11,\n",
       "                    10, 6, 14, 2, 12],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[8, 7, 5, 6, 10, 2, 10, 1, 2, 1, 10, 1, 2, 2, 4, 2, 10,\n",
       "                    9, 7, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'min_samples_split': 19,\n",
       "   'min_samples_leaf': 19,\n",
       "   'max_features': 8},\n",
       "  {'min_samples_split': 13, 'min_samples_leaf': 10, 'max_features': 7},\n",
       "  {'min_samples_split': 16, 'min_samples_leaf': 8, 'max_features': 5},\n",
       "  {'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 6},\n",
       "  {'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 10},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 12, 'max_features': 2},\n",
       "  {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 10},\n",
       "  {'min_samples_split': 13, 'min_samples_leaf': 9, 'max_features': 1},\n",
       "  {'min_samples_split': 12, 'min_samples_leaf': 11, 'max_features': 2},\n",
       "  {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 1},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 10},\n",
       "  {'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 1},\n",
       "  {'min_samples_split': 9, 'min_samples_leaf': 11, 'max_features': 2},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 2},\n",
       "  {'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 4},\n",
       "  {'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 2},\n",
       "  {'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 10},\n",
       "  {'min_samples_split': 2, 'min_samples_leaf': 14, 'max_features': 9},\n",
       "  {'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 7},\n",
       "  {'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': 9}],\n",
       " 'split0_test_score': array([-0.23386659, -0.16948338, -0.16202578, -0.158375  , -0.17504702,\n",
       "        -0.24509451, -0.24373447, -0.28424973, -0.2392173 , -0.34143321,\n",
       "        -0.2039565 , -0.26147609, -0.2392173 , -0.21413061, -0.19001857,\n",
       "        -0.23185586, -0.15195581, -0.19508324, -0.14732012, -0.18130047]),\n",
       " 'split1_test_score': array([-0.17514287, -0.13978361, -0.13727699, -0.13236705, -0.14356995,\n",
       "        -0.21578927, -0.18656735, -0.25767226, -0.21045568, -0.29937112,\n",
       "        -0.15746293, -0.22902326, -0.21045568, -0.18536987, -0.15726795,\n",
       "        -0.20488489, -0.13688377, -0.15269231, -0.13225069, -0.14555882]),\n",
       " 'split2_test_score': array([-0.16536027, -0.1145551 , -0.11570245, -0.10073457, -0.12089233,\n",
       "        -0.20446736, -0.17496336, -0.25342193, -0.1977555 , -0.30342779,\n",
       "        -0.13769588, -0.23603485, -0.1977555 , -0.17125577, -0.13767642,\n",
       "        -0.19205141, -0.1000834 , -0.13179835, -0.09562357, -0.12269657]),\n",
       " 'split3_test_score': array([-0.26629675, -0.20078553, -0.19227712, -0.18160158, -0.20941862,\n",
       "        -0.27477838, -0.28353971, -0.31403444, -0.267723  , -0.37350209,\n",
       "        -0.24011787, -0.28195207, -0.267723  , -0.2328811 , -0.22295924,\n",
       "        -0.26094194, -0.17285337, -0.23037446, -0.16704637, -0.21786362]),\n",
       " 'split4_test_score': array([-0.25770832, -0.20077539, -0.19493012, -0.17998111, -0.20060197,\n",
       "        -0.25534237, -0.26835756, -0.28685943, -0.25185824, -0.33387506,\n",
       "        -0.21958461, -0.26693176, -0.25185824, -0.22541689, -0.21589835,\n",
       "        -0.246252  , -0.17698101, -0.21643134, -0.16999177, -0.20679405]),\n",
       " 'mean_test_score': array([-0.21967496, -0.1650766 , -0.16044249, -0.15061186, -0.16990598,\n",
       "        -0.23909438, -0.23143249, -0.27924756, -0.23340195, -0.33032186,\n",
       "        -0.19176356, -0.25508361, -0.23340195, -0.20581085, -0.18476411,\n",
       "        -0.22719722, -0.14775147, -0.18527594, -0.1424465 , -0.17484271]),\n",
       " 'std_test_score': array([0.04184429, 0.03394439, 0.03080149, 0.03067181, 0.03354354,\n",
       "        0.02575075, 0.0434318 , 0.02202519, 0.02588277, 0.02713617,\n",
       "        0.03836839, 0.01972386, 0.02588277, 0.02365714, 0.0329518 ,\n",
       "        0.02552082, 0.02790161, 0.03747603, 0.02714402, 0.0360282 ]),\n",
       " 'rank_test_score': array([12,  5,  4,  3,  6, 17, 14, 19, 15, 20, 10, 18, 15, 11,  8, 13,  2,\n",
       "         9,  1,  7])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Mean Test Score: -0.21967496002005796, Parameters: {'min_samples_split': 19, 'min_samples_leaf': 19, 'max_features': 8}\n",
      "Iteration 1 - Mean Test Score: -0.16507660123457998, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 10, 'max_features': 7}\n",
      "Iteration 2 - Mean Test Score: -0.1604424932683084, Parameters: {'min_samples_split': 16, 'min_samples_leaf': 8, 'max_features': 5}\n",
      "Iteration 3 - Mean Test Score: -0.1506118625798829, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 6}\n",
      "Iteration 4 - Mean Test Score: -0.16990598044204633, Parameters: {'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 10}\n",
      "Iteration 5 - Mean Test Score: -0.23909437831753796, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 12, 'max_features': 2}\n",
      "Iteration 6 - Mean Test Score: -0.23143249183877268, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 10}\n",
      "Iteration 7 - Mean Test Score: -0.27924755819356184, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 9, 'max_features': 1}\n",
      "Iteration 8 - Mean Test Score: -0.23340194562198896, Parameters: {'min_samples_split': 12, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration 9 - Mean Test Score: -0.33032185547313364, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 1}\n",
      "Iteration 10 - Mean Test Score: -0.19176355514147775, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 10}\n",
      "Iteration 11 - Mean Test Score: -0.25508360555813037, Parameters: {'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 1}\n",
      "Iteration 12 - Mean Test Score: -0.23340194562198896, Parameters: {'min_samples_split': 9, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration 13 - Mean Test Score: -0.20581084994714144, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 2}\n",
      "Iteration 14 - Mean Test Score: -0.18476410513507086, Parameters: {'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 4}\n",
      "Iteration 15 - Mean Test Score: -0.22719722072399656, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 2}\n",
      "Iteration 16 - Mean Test Score: -0.14775147238757366, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 10}\n",
      "Iteration 17 - Mean Test Score: -0.18527594012122553, Parameters: {'min_samples_split': 2, 'min_samples_leaf': 14, 'max_features': 9}\n",
      "Iteration 18 - Mean Test Score: -0.1424465043852196, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 7}\n",
      "Iteration 19 - Mean Test Score: -0.17484270504342433, Parameters: {'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': 9}\n"
     ]
    }
   ],
   "source": [
    "for index, (mean_score, params) in enumerate(zip(results_stratified[\"mean_test_score\"], results_stratified[\"params\"])):\n",
    "    print(f\"Iteration {index} - Mean Test Score: {mean_score}, Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Mean Test Score: -0.21089212802635998, Parameters: {'min_samples_split': 19, 'min_samples_leaf': 19, 'max_features': 8}\n",
      "Iteration 1 - Mean Test Score: -0.16126511127811952, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 10, 'max_features': 7}\n",
      "Iteration 2 - Mean Test Score: -0.15773293251844192, Parameters: {'min_samples_split': 16, 'min_samples_leaf': 8, 'max_features': 5}\n",
      "Iteration 3 - Mean Test Score: -0.14563733723230055, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 6}\n",
      "Iteration 4 - Mean Test Score: -0.16567433663782424, Parameters: {'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 10}\n",
      "Iteration 5 - Mean Test Score: -0.24255398936875813, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 12, 'max_features': 2}\n",
      "Iteration 6 - Mean Test Score: -0.21956869342455643, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 10}\n",
      "Iteration 7 - Mean Test Score: -0.28548234553308954, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 9, 'max_features': 1}\n",
      "Iteration 8 - Mean Test Score: -0.23636724116557958, Parameters: {'min_samples_split': 12, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration 9 - Mean Test Score: -0.3387955964696673, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 1}\n",
      "Iteration 10 - Mean Test Score: -0.18667064783861975, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 10}\n",
      "Iteration 11 - Mean Test Score: -0.25929223225798936, Parameters: {'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 1}\n",
      "Iteration 12 - Mean Test Score: -0.23636724116557958, Parameters: {'min_samples_split': 9, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration 13 - Mean Test Score: -0.20916324423194071, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 2}\n",
      "Iteration 14 - Mean Test Score: -0.1841218979210851, Parameters: {'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 4}\n",
      "Iteration 15 - Mean Test Score: -0.23085835081567113, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 2}\n",
      "Iteration 16 - Mean Test Score: -0.14027568622053171, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 10}\n",
      "Iteration 17 - Mean Test Score: -0.18118093122875883, Parameters: {'min_samples_split': 2, 'min_samples_leaf': 14, 'max_features': 9}\n",
      "Iteration 18 - Mean Test Score: -0.13596947313800864, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 7}\n",
      "Iteration 19 - Mean Test Score: -0.17120585892680787, Parameters: {'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': 9}\n"
     ]
    }
   ],
   "source": [
    "for index, (mean_score, params) in enumerate(zip(results_unstratified['mean_test_score'],\n",
    "                                                results_unstratified[\"params\"])):\n",
    "    print(f\"Iteration {index} - Mean Test Score: {mean_score}, Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 4.1e-05\n"
     ]
    }
   ],
   "source": [
    "print(round(np.var(results_unstratified['std_test_score']), 6), float(round(np.var(results_stratified['std_test_score']), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choplip (parallelisierung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Mean Test Score: 0.1820069225212941, Parameters: {'min_samples_split': 19, 'min_samples_leaf': 19, 'max_features': 8}\n",
      "Iteration 1 - Mean Test Score: 0.14134281792383813, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 10, 'max_features': 7}\n",
      "Iteration 2 - Mean Test Score: 0.13979710330524364, Parameters: {'min_samples_split': 16, 'min_samples_leaf': 8, 'max_features': 5}\n",
      "Iteration 3 - Mean Test Score: 0.12663663941987063, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 6}\n",
      "Iteration 4 - Mean Test Score: 0.14083267857546644, Parameters: {'min_samples_split': 8, 'min_samples_leaf': 11, 'max_features': 10}\n",
      "Iteration 5 - Mean Test Score: 0.235536751927918, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 12, 'max_features': 2}\n",
      "Iteration 6 - Mean Test Score: 0.18506289664078565, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 10}\n",
      "Iteration 7 - Mean Test Score: 0.28783470953072376, Parameters: {'min_samples_split': 13, 'min_samples_leaf': 9, 'max_features': 1}\n",
      "Iteration 8 - Mean Test Score: 0.22943321027885152, Parameters: {'min_samples_split': 12, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration 9 - Mean Test Score: 0.3399945854302898, Parameters: {'min_samples_split': 18, 'min_samples_leaf': 20, 'max_features': 1}\n",
      "Iteration 10 - Mean Test Score: 0.162355233728231, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 15, 'max_features': 10}\n",
      "Iteration 11 - Mean Test Score: 0.2525178393037241, Parameters: {'min_samples_split': 17, 'min_samples_leaf': 1, 'max_features': 1}\n",
      "Iteration 12 - Mean Test Score: 0.22943321027885152, Parameters: {'min_samples_split': 9, 'min_samples_leaf': 11, 'max_features': 2}\n",
      "Iteration 13 - Mean Test Score: 0.1959589615824866, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 2}\n",
      "Iteration 14 - Mean Test Score: 0.16806468951659584, Parameters: {'min_samples_split': 5, 'min_samples_leaf': 11, 'max_features': 4}\n",
      "Iteration 15 - Mean Test Score: 0.222631421897699, Parameters: {'min_samples_split': 20, 'min_samples_leaf': 10, 'max_features': 2}\n",
      "Iteration 16 - Mean Test Score: 0.11868229342172378, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 10}\n",
      "Iteration 17 - Mean Test Score: 0.15759405339245372, Parameters: {'min_samples_split': 2, 'min_samples_leaf': 14, 'max_features': 9}\n",
      "Iteration 18 - Mean Test Score: 0.11787719412389495, Parameters: {'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 7}\n",
      "Iteration 19 - Mean Test Score: 0.14708141367134187, Parameters: {'min_samples_split': 6, 'min_samples_leaf': 12, 'max_features': 9}\n"
     ]
    }
   ],
   "source": [
    "# Get the best model from the RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "results_stratified = random_search.cv_results_\n",
    "mse_list_stratified = []\n",
    "\n",
    "# Iterate through the results and make predictions for each iteration\n",
    "for index, (mean_score, params) in enumerate(zip(results_stratified[\"mean_test_score\"], results_stratified[\"params\"])):\n",
    "   \n",
    "    best_model.set_params(**params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_list_stratified.append(mse)\n",
    "    print(f\"Iteration {index} - MSE: {mse}, Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_list_stratified==mse_list_unstratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_stratified[\"params\"] == results_unstratified[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_optimization(X_train, \n",
    "                              y_train, \n",
    "                              X_test, \n",
    "                              y_test, \n",
    "                              cv, \n",
    "                              n_groups, \n",
    "                              scoring, \n",
    "                              n_jobs, \n",
    "                              n_iter, \n",
    "                              random_state,\n",
    "                              stratified):\n",
    "        '''\n",
    "        Function to perform the optimization.\n",
    "        Inputs:\n",
    "            the same as in optimize function\n",
    "            stratified: whether to use stratified k-fold or not\n",
    "        Outputs:\n",
    "            evaluation_results: the evaluation results in a dictionary\n",
    "            best_params: the best parameters in a dictionary\n",
    "        '''\n",
    "        if stratified:\n",
    "            cv_splits = create_cont_folds(y_train, n_folds=cv, n_groups=n_groups)\n",
    "            output_text = 'Stratified Split Cross-validation'\n",
    "        else:\n",
    "            cv_splits = cv\n",
    "            output_text = 'Random Split Cross-validation'\n",
    "        \n",
    "        start_time = time.time()\n",
    "        random_search = RandomizedSearchCV(estimator=model,\n",
    "                                           param_distributions=RF_param_grid,\n",
    "                                           n_iter=n_iter,\n",
    "                                           cv=cv_splits,\n",
    "                                           scoring=scoring,\n",
    "                                           n_jobs=n_jobs,\n",
    "                                           random_state=random_state)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        running_time = end_time - start_time\n",
    "        print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "        # Evaluate the model\n",
    "        evaluation_results = evaluate_rf(random_search, X_train, X_test, y_train, y_test)\n",
    "        print(\"Evaluation Results of\", output_text, ': ', evaluation_results)\n",
    "        print('running_time: ', round(running_time/60, 2), ' min')\n",
    "        \n",
    "        return evaluation_results, random_search.best_params_, running_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_cont_folds(y, \n",
    "                          n_folds=5, \n",
    "                          n_groups=5, \n",
    "                          seed=1):\n",
    "        '''\n",
    "        Function to create continuous folds.\n",
    "        Inputs:\n",
    "            y: the target variable\n",
    "            n_folds: the number of folds\n",
    "            n_groups: the number of groups (based on quantiles)\n",
    "            seed: the seed to be used\n",
    "        Outputs:\n",
    "            cv_splits: the indices for the folds\n",
    "        '''\n",
    "        # create StratifiedKFold like for classification\n",
    "        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "        # create groups in y with pd.qcut: quantile-based discretization \n",
    "        y_grouped = pd.qcut(y, n_groups, labels=False)\n",
    "\n",
    "        # create fold numbers    \n",
    "        fold_nums = np.zeros(len(y))\n",
    "        #split(X, y[, groups]): Generate indices to split data into training and test set\n",
    "        for fold_no, (t, v) in enumerate(skf.split(y_grouped, y_grouped)): #@Nadja: unabhängig von n_folds? n_folds = fol_no, test_data_size = N/n_folds\n",
    "            fold_nums[v] = fold_no\n",
    "\n",
    "        cv_splits = []\n",
    "\n",
    "        # iterate over folds and creat train and test indices for each fold\n",
    "        for i in range(n_folds):\n",
    "            test_indices = np.argwhere(fold_nums==i).flatten()\n",
    "            train_indices = list(set(range(len(y_grouped))) - set(test_indices))\n",
    "            cv_splits.append((train_indices, test_indices))\n",
    "\n",
    "        return cv_splits\n",
    "\n",
    "def evaluate_rf(model, X_train, X_test, y_train, y_test):\n",
    "        '''\n",
    "        Function to evaluate the model.\n",
    "        Inputs:\n",
    "            model: the model to be evaluated\n",
    "            X_train, X_test, y_train, y_test: the train and test data\n",
    "        Outputs:\n",
    "            dictionary with the evaluation results (R2, MSE, MAE)\n",
    "        '''\n",
    "        model=model.best_estimator_\n",
    "        # @Anne: This somehow also does not work, do not know why.\n",
    "        #best_score = model.best_score_\n",
    "\n",
    "        train_r2, test_r2 = round(model.score(X_train, y_train), 4), round(model.score(X_test, y_test), 4)\n",
    "        y_train_pred, y_test_pred = model.predict(X_train), model.predict(X_test)\n",
    "        train_mse, test_mse = round(mean_squared_error(y_train, y_train_pred), 4), round(mean_squared_error(y_test, y_test_pred), 4)\n",
    "        train_mae, test_mae = round(mean_absolute_error(y_train, y_train_pred), 4), round(mean_absolute_error(y_test, y_test_pred), 4)\n",
    "        return {'train r2': train_r2, \n",
    "                'test r2': test_r2, \n",
    "                'train mse': train_mse,\n",
    "                'test mse': test_mse,\n",
    "                'train mae': train_mae,\n",
    "                'test mae': test_mae}\n",
    "        \n",
    "     \n",
    "def _convert_numpy_types(obj):\n",
    "        '''\n",
    "        Function to convert numpy types.\n",
    "        Inputs:\n",
    "            obj: the object to be converted\n",
    "        Outputs:\n",
    "            the converted object\n",
    "        '''\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.int32):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [_convert_numpy_types(item) for item in obj]\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: _convert_numpy_types(value) for key, value in obj.items()}\n",
    "        else:\n",
    "            return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_friedman1(n_samples, n_features, noise, random_state, transformation='identity'):\n",
    "        '''\n",
    "        Function to generate dataset according to Friedman1.\n",
    "        Inputs:\n",
    "            n_samples: number of data points\n",
    "            n_features: number of features (have to be at least 5)\n",
    "            noise: The standard deviation of the gaussian noise applied to the output.\n",
    "            random_state: to repreoduce dataset\n",
    "        Outputs:\n",
    "            features: array\n",
    "            y: array\n",
    "\n",
    "        '''\n",
    "        features, y = make_friedman1(n_samples=n_samples, \n",
    "                                    n_features=n_features, \n",
    "                                    noise=noise, \n",
    "                                    random_state=random_state)\n",
    "        if transformation=='identity':\n",
    "            pass\n",
    "        elif transformation == 'log':\n",
    "            y = np.log(y)\n",
    "             \n",
    "        elif transformation == 'sqrt':\n",
    "            y = np.sqrt(y)\n",
    "             \n",
    "        else:\n",
    "            raise ValueError('Transformation not implemented.')\n",
    "\n",
    "        return features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(    params,\n",
    "                 data='friedman',\n",
    "                 random_states=None):\n",
    "        '''\n",
    "        Function to optimize the model.\n",
    "        Inputs:\n",
    "            X_train, X_test, y_train, y_test: the train and test data\n",
    "            cv: the number of folds\n",
    "            n_groups: the number of groups (based on quantiles)\n",
    "            scoring: the scoring to be used\n",
    "            n_jobs: the number of jobs to be used\n",
    "            n_iter: the number of iterations\n",
    "            ROOT_PATH: the root path to be used (results are stored in JSON file)\n",
    "            transformation: the transformation of target to be applied\n",
    "        Outputs:\n",
    "            unstratified_results: the results of the unstratified cross-validation\n",
    "            stratified_results: the results of the stratified cross-validation\n",
    "        Important: The results are stored in a JSON file. Initialize a new file with an empty list as content.\n",
    "        '''\n",
    "                                                     \n",
    "        \n",
    "        # get parameters from params dictionary\n",
    "        n_train = params['n_train']\n",
    "        n_test = params['n_test']\n",
    "        n_features = params['n_features']\n",
    "        noise = params['FD_noise']\n",
    "        transformation = params['transformation']\n",
    "        n_folds= params['n_folds']\n",
    "        n_groups = params['n_groups']\n",
    "        scoring = params['scoring']\n",
    "        n_jobs = params['n_jobs']\n",
    "        n_iter = params['n_iter']\n",
    "        n_repetitions = params['n_repetitions']\n",
    "        json_file = params['json_file']\n",
    "\n",
    "        if data == 'friedman':\n",
    "            # maybe implement accessing and generating the data nicer\n",
    "            X_test, y_test = generate_friedman1(n_samples=n_test,\n",
    "                                                     n_features=n_features,\n",
    "                                                     noise=noise,\n",
    "                                                     random_state=1718,\n",
    "                                                     transformation=transformation)\n",
    "        # @Anne und auch @Nadja: macht das Sinn?\n",
    "        if not isinstance(random_states, list):\n",
    "            random.seed(seed)\n",
    "            random_states = random.sample(range(1, 10000), n_repetitions)\n",
    "\n",
    "        else:\n",
    "            random_states = random_states[:n_repetitions]\n",
    "\n",
    "        print(\"RandomizesdSearchCV with params n_folds =\",\n",
    "            n_folds, \", ngroups: \", n_groups, \", scoring: \",scoring, \", n_jobs: \",n_jobs,\n",
    "            \", n_iter: \", n_iter, \" and save to  \", json_file, \"\\n\")\n",
    "        \n",
    "        all_results = {}\n",
    "        all_results_stratified = {}\n",
    "\n",
    "        initialization = {\n",
    "            'model_info': params,\n",
    "            'seed': seed\n",
    "        }\n",
    "        for repetition in range(n_repetitions):\n",
    "            if data == 'friedman':\n",
    "                X_train, y_train = generate_friedman1(n_samples=n_train,\n",
    "                                                        n_features=n_features,\n",
    "                                                        noise=noise,\n",
    "                                                        random_state=random_states[repetition],\n",
    "                                                        transformation=transformation)\n",
    "            # Check for NaN values in the data\n",
    "            if np.isnan(y_train).any() or np.isnan(y_test).any(): \n",
    "                X_train, y_train = make_friedman1(n_samples=n_train,\n",
    "                                    n_features=n_features, \n",
    "                                    noise=noise, \n",
    "                                    random_state=random_states[repetition])\n",
    "                X_test, y_test = make_friedman1(n_samples=n_test,\n",
    "                                    n_features=n_features,\n",
    "                                    noise=noise,\n",
    "                                    random_state=1718)\n",
    "                min_val = min(y_train.min(), y_test.min())\n",
    "                # @Anne: Noch absprechen... +1 weil es sonst Skala durch Werte [0, 1] hauptsächlich bei y_test größer wird.           \n",
    "                y_train = y_train + abs(min_val) + 1\n",
    "                y_test = y_test + abs(min_val) + 1\n",
    "                if transformation=='identity':\n",
    "                    pass\n",
    "                elif transformation == 'log':\n",
    "                    y_train = np.log(y_train)\n",
    "                    y_test = np.log(y_test)\n",
    "                elif transformation == 'sqrt':\n",
    "                    y_train = np.sqrt(y_train)\n",
    "                    y_test = np.sqrt(y_test)\n",
    "\n",
    "            # Perform optimization with unstratified cross-validation\n",
    "            unstratified_results, unstratified_params, unstratified_running_time = perform_optimization(X_train, \n",
    "                                                            y_train, \n",
    "                                                            X_test,\n",
    "                                                            y_test,\n",
    "                                                            n_folds, \n",
    "                                                            n_groups,\n",
    "                                                            scoring, \n",
    "                                                            n_jobs, \n",
    "                                                            n_iter, \n",
    "                                                            random_states[repetition],\n",
    "                                                            stratified=False)\n",
    "            all_results.update({f'Repetition {repetition}': unstratified_results})\n",
    "\n",
    "            # Perform optimization with stratified cross-validation\n",
    "            stratified_results, stratified_params, stratified_running_time = perform_optimization(X_train, \n",
    "                                                            y_train, \n",
    "                                                            X_test,\n",
    "                                                            y_test,\n",
    "                                                            n_folds, \n",
    "                                                            n_groups, \n",
    "                                                            scoring, \n",
    "                                                            n_jobs, \n",
    "                                                            n_iter, \n",
    "                                                            random_states[repetition],\n",
    "                                                            stratified=True)\n",
    "            all_results_stratified.update({f'Repetition {repetition}': stratified_results}) # @Anne: check ich nicht ganz, was das macht\n",
    "\n",
    "            if unstratified_params == stratified_params:\n",
    "                hyperparameters_same = True\n",
    "            else:\n",
    "                hyperparameters_same = False\n",
    "            \n",
    "            # Save results and parameters to a file\n",
    "            results = {\n",
    "                'repetition': repetition,\n",
    "                'random_state': random_states[repetition],\n",
    "                'hyperparameters_same': hyperparameters_same,\n",
    "                'unstratified_params':unstratified_params,\n",
    "                'stratified_params': stratified_params,\n",
    "                'unstratified_results': unstratified_results,\n",
    "                'stratified_results': stratified_results,\n",
    "                'unstratified_running_time': round(unstratified_running_time,2), \n",
    "                'stratified_running_time': round(stratified_running_time, 2)\n",
    "            }\n",
    "\n",
    "            initialization.update(results)\n",
    "            # Load existing data or create an empty list\n",
    "            with open(json_file, 'r') as file:\n",
    "                existing_data = json.load(file)\n",
    "\n",
    "            # Append the new results dictionary to the existing data\n",
    "            existing_data.append(initialization)\n",
    "\n",
    "            # Write the updated data back to the JSON file\n",
    "            with open(json_file, 'w') as file:\n",
    "                json.dump(existing_data, file, indent=4, default=_convert_numpy_types)\n",
    "\n",
    "        return all_results, all_results_stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_slds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
