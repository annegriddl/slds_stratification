{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Data Loading\n",
    "In this notebook we load the data from the json-files for RF and XGB respectively and do som general data checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load json data for RF and XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from utils_evaluation import flatten_data\n",
    "import numpy as np\n",
    "#from utils_evaluation import flatten_data, generate_hyperparameter_combinations_dict, descreptives, save_results_to_csv, csv_to_list, flatten_nested_lists, error_estimator, grouped_bar_plot_hyperparameters\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "# remove waringns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "#### First: Random Forest\n",
    "# set file path to json files (change model) -> get names of json files\n",
    "json_path = \"../results/rf/\"\n",
    "json_files = os.listdir(json_path)\n",
    "\n",
    "\n",
    "# load data from json file\n",
    "data = []\n",
    "for file in json_files:\n",
    "    with open(json_path + file) as f:\n",
    "        data_new = json.load(f)\n",
    "        data = data + data_new\n",
    "\n",
    "# flatten dictionary and convert to dataframe\n",
    "data_all_flatten, keys_dic = flatten_data(data)\n",
    "df_rf = pd.DataFrame(data_all_flatten)\n",
    "print('Shape of data', df_rf.shape)\n",
    "print('Colum names of data', df_rf.columns)\n",
    "print('Number of Colums', df_rf.columns.shape)\n",
    "print('Keys of dictionary (', len(keys_dic), ')', keys_dic)\n",
    "\n",
    "\n",
    "# check for duplicates ind data\n",
    "print('Duplicates: ', df_rf.astype(str).duplicated().sum())\n",
    "\n",
    "\n",
    "df_rf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (4800, 87)\n",
      "Colum names of data Index(['model_info_model', 'model_info_n_train', 'model_info_n_test',\n",
      "       'model_info_n_features', 'model_info_noise',\n",
      "       'model_info_transformation', 'model_info_group_size',\n",
      "       'model_info_n_folds', 'model_info_n_iter', 'model_info_n_repetitions',\n",
      "       'model_info_scoring', 'model_info_n_jobs', 'model_info_json_file',\n",
      "       'repetition', 'random_state', 'hyperparameters_same',\n",
      "       'unstratified_results_train r2', 'unstratified_results_test r2',\n",
      "       'unstratified_results_train mse', 'unstratified_results_test mse',\n",
      "       'unstratified_results_train mae', 'unstratified_results_test mae',\n",
      "       'stratified_results_train r2', 'stratified_results_test r2',\n",
      "       'stratified_results_train mse', 'stratified_results_test mse',\n",
      "       'stratified_results_train mae', 'stratified_results_test mae',\n",
      "       'cv_unstratified_iterations_mean_fit_time',\n",
      "       'cv_unstratified_iterations_std_fit_time',\n",
      "       'cv_unstratified_iterations_mean_score_time',\n",
      "       'cv_unstratified_iterations_std_score_time',\n",
      "       'cv_unstratified_iterations_param_subsample',\n",
      "       'cv_unstratified_iterations_param_min_child_weight',\n",
      "       'cv_unstratified_iterations_param_max_depth',\n",
      "       'cv_unstratified_iterations_param_learning_rate',\n",
      "       'cv_unstratified_iterations_param_gamma',\n",
      "       'cv_unstratified_iterations_param_colsample_bytree',\n",
      "       'cv_unstratified_iterations_params',\n",
      "       'cv_unstratified_iterations_split0_test_score',\n",
      "       'cv_unstratified_iterations_split1_test_score',\n",
      "       'cv_unstratified_iterations_split2_test_score',\n",
      "       'cv_unstratified_iterations_split3_test_score',\n",
      "       'cv_unstratified_iterations_split4_test_score',\n",
      "       'cv_unstratified_iterations_mean_test_score',\n",
      "       'cv_unstratified_iterations_std_test_score',\n",
      "       'cv_unstratified_iterations_rank_test_score',\n",
      "       'cv_stratified_iterations_mean_fit_time',\n",
      "       'cv_stratified_iterations_std_fit_time',\n",
      "       'cv_stratified_iterations_mean_score_time',\n",
      "       'cv_stratified_iterations_std_score_time',\n",
      "       'cv_stratified_iterations_param_subsample',\n",
      "       'cv_stratified_iterations_param_min_child_weight',\n",
      "       'cv_stratified_iterations_param_max_depth',\n",
      "       'cv_stratified_iterations_param_learning_rate',\n",
      "       'cv_stratified_iterations_param_gamma',\n",
      "       'cv_stratified_iterations_param_colsample_bytree',\n",
      "       'cv_stratified_iterations_params',\n",
      "       'cv_stratified_iterations_split0_test_score',\n",
      "       'cv_stratified_iterations_split1_test_score',\n",
      "       'cv_stratified_iterations_split2_test_score',\n",
      "       'cv_stratified_iterations_split3_test_score',\n",
      "       'cv_stratified_iterations_split4_test_score',\n",
      "       'cv_stratified_iterations_mean_test_score',\n",
      "       'cv_stratified_iterations_std_test_score',\n",
      "       'cv_stratified_iterations_rank_test_score',\n",
      "       'cv_iteration_refit_test_r2', 'cv_iteration_refit_test_mse',\n",
      "       'cv_iteration_refit_test_mae',\n",
      "       'cv_folds_descriptives_unstratified_ks_statistic',\n",
      "       'cv_folds_descriptives_unstratified_p_value',\n",
      "       'cv_folds_descriptives_unstratified_intersection_area',\n",
      "       'cv_folds_descriptives_stratified_ks_statistic',\n",
      "       'cv_folds_descriptives_stratified_p_value',\n",
      "       'cv_folds_descriptives_stratified_intersection_area',\n",
      "       'unstratified_best_params_subsample',\n",
      "       'unstratified_best_params_min_child_weight',\n",
      "       'unstratified_best_params_max_depth',\n",
      "       'unstratified_best_params_learning_rate',\n",
      "       'unstratified_best_params_gamma',\n",
      "       'unstratified_best_params_colsample_bytree',\n",
      "       'stratified_best_params_subsample',\n",
      "       'stratified_best_params_min_child_weight',\n",
      "       'stratified_best_params_max_depth',\n",
      "       'stratified_best_params_learning_rate', 'stratified_best_params_gamma',\n",
      "       'stratified_best_params_colsample_bytree'],\n",
      "      dtype='object')\n",
      "Number of Colums (87,)\n",
      "Keys of dictionary ( 10 ) ['model_info', 'cv_folds_descriptives_unstratified', 'unstratified_results', 'cv_folds_descriptives_stratified', 'stratified_best_params', 'cv_iteration_refit_test', 'unstratified_best_params', 'cv_unstratified_iterations', 'cv_stratified_iterations', 'stratified_results']\n",
      "Duplicates:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_info_model</th>\n",
       "      <th>model_info_n_train</th>\n",
       "      <th>model_info_n_test</th>\n",
       "      <th>model_info_n_features</th>\n",
       "      <th>model_info_noise</th>\n",
       "      <th>model_info_transformation</th>\n",
       "      <th>model_info_group_size</th>\n",
       "      <th>model_info_n_folds</th>\n",
       "      <th>model_info_n_iter</th>\n",
       "      <th>model_info_n_repetitions</th>\n",
       "      <th>...</th>\n",
       "      <th>unstratified_best_params_max_depth</th>\n",
       "      <th>unstratified_best_params_learning_rate</th>\n",
       "      <th>unstratified_best_params_gamma</th>\n",
       "      <th>unstratified_best_params_colsample_bytree</th>\n",
       "      <th>stratified_best_params_subsample</th>\n",
       "      <th>stratified_best_params_min_child_weight</th>\n",
       "      <th>stratified_best_params_max_depth</th>\n",
       "      <th>stratified_best_params_learning_rate</th>\n",
       "      <th>stratified_best_params_gamma</th>\n",
       "      <th>stratified_best_params_colsample_bytree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgb</td>\n",
       "      <td>200</td>\n",
       "      <td>100000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>identity</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.687279</td>\n",
       "      <td>0.847472</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.311333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xgb</td>\n",
       "      <td>200</td>\n",
       "      <td>100000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>identity</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>3.786479</td>\n",
       "      <td>0.982669</td>\n",
       "      <td>0.729761</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222667</td>\n",
       "      <td>3.786479</td>\n",
       "      <td>0.982669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xgb</td>\n",
       "      <td>200</td>\n",
       "      <td>100000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>identity</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>1.945888</td>\n",
       "      <td>0.548831</td>\n",
       "      <td>0.997877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.585893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xgb</td>\n",
       "      <td>200</td>\n",
       "      <td>100000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>identity</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222667</td>\n",
       "      <td>1.945888</td>\n",
       "      <td>0.982669</td>\n",
       "      <td>0.997877</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.517950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xgb</td>\n",
       "      <td>200</td>\n",
       "      <td>100000</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>identity</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.178333</td>\n",
       "      <td>1.394951</td>\n",
       "      <td>0.976185</td>\n",
       "      <td>0.697330</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.222667</td>\n",
       "      <td>1.394951</td>\n",
       "      <td>0.548831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_info_model  model_info_n_train  model_info_n_test  \\\n",
       "0              xgb                 200             100000   \n",
       "1              xgb                 200             100000   \n",
       "2              xgb                 200             100000   \n",
       "3              xgb                 200             100000   \n",
       "4              xgb                 200             100000   \n",
       "\n",
       "   model_info_n_features  model_info_noise model_info_transformation  \\\n",
       "0                      8                 0                  identity   \n",
       "1                      8                 0                  identity   \n",
       "2                      8                 0                  identity   \n",
       "3                      8                 0                  identity   \n",
       "4                      8                 0                  identity   \n",
       "\n",
       "   model_info_group_size  model_info_n_folds  model_info_n_iter  \\\n",
       "0                      5                   5                200   \n",
       "1                      5                   5                200   \n",
       "2                      5                   5                200   \n",
       "3                      5                   5                200   \n",
       "4                      5                   5                200   \n",
       "\n",
       "   model_info_n_repetitions  ... unstratified_best_params_max_depth  \\\n",
       "0                        20  ...                                  2   \n",
       "1                        20  ...                                  2   \n",
       "2                        20  ...                                  2   \n",
       "3                        20  ...                                  2   \n",
       "4                        20  ...                                 17   \n",
       "\n",
       "   unstratified_best_params_learning_rate unstratified_best_params_gamma  \\\n",
       "0                                0.222667                       1.000000   \n",
       "1                                0.267000                       3.786479   \n",
       "2                                0.267000                       1.945888   \n",
       "3                                0.222667                       1.945888   \n",
       "4                                0.178333                       1.394951   \n",
       "\n",
       "   unstratified_best_params_colsample_bytree  \\\n",
       "0                                   0.687279   \n",
       "1                                   0.982669   \n",
       "2                                   0.548831   \n",
       "3                                   0.982669   \n",
       "4                                   0.976185   \n",
       "\n",
       "   stratified_best_params_subsample  stratified_best_params_min_child_weight  \\\n",
       "0                          0.847472                                        2   \n",
       "1                          0.729761                                        6   \n",
       "2                          0.997877                                        2   \n",
       "3                          0.997877                                        4   \n",
       "4                          0.697330                                        6   \n",
       "\n",
       "   stratified_best_params_max_depth  stratified_best_params_learning_rate  \\\n",
       "0                                 2                              0.311333   \n",
       "1                                 2                              0.222667   \n",
       "2                                 2                              0.222667   \n",
       "3                                 2                              0.134000   \n",
       "4                                 3                              0.222667   \n",
       "\n",
       "   stratified_best_params_gamma  stratified_best_params_colsample_bytree  \n",
       "0                      1.000000                                 0.982669  \n",
       "1                      3.786479                                 0.982669  \n",
       "2                      1.000000                                 0.585893  \n",
       "3                      1.000000                                 0.517950  \n",
       "4                      1.394951                                 0.548831  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Second: XGBoost\n",
    "# set file path to json files (change model) -> get names of json files\n",
    "json_path = \"../results/xgb/\"\n",
    "json_files = os.listdir(json_path)\n",
    "\n",
    "\n",
    "# load data from json file\n",
    "data = []\n",
    "for file in json_files:\n",
    "    with open(json_path + file) as f:\n",
    "        data_new = json.load(f)\n",
    "        data = data + data_new\n",
    "\n",
    "# flatten dictionary and convert to dataframe\n",
    "data_all_flatten, keys_dic = flatten_data(data)\n",
    "df_xgb = pd.DataFrame(data_all_flatten)\n",
    "print('Shape of data', df_xgb.shape)\n",
    "print('Colum names of data', df_xgb.columns)\n",
    "print('Number of Colums', df_xgb.columns.shape)\n",
    "print('Keys of dictionary (', len(keys_dic), ')', keys_dic)\n",
    "\n",
    "\n",
    "# check for duplicates ind data\n",
    "print('Duplicates: ', df_xgb.astype(str).duplicated().sum())\n",
    "\n",
    "\n",
    "df_xgb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df_rf, df_xgb], axis=0, ignore_index=True)\n",
    "original_shape = data.shape[0]\n",
    "print('Shape of data', original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opitonal: exclude all rows if hyperparmeters are the same\n",
    "exclude_hyp_dame = False\n",
    "if exclude_hyp_dame:\n",
    "    data = data[data['hyperparameters_same'] == False]\n",
    "    data = data.reset_index(drop=True) #reset index of data\n",
    "    print('Original shape: ', original_shape)\n",
    "    print('Shape after removing hyperparameters that are the same: ', data.shape[0])\n",
    "    print('Percetage', data.shape[0]/original_shape)\n",
    "    data.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (9600, 104)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    200_0_5\n",
       "1    200_0_5\n",
       "2    200_0_5\n",
       "3    200_0_5\n",
       "4    200_0_5\n",
       "Name: param_model, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creat unique identifier for plots\n",
    "model_vars = ['model_info_n_train',\t'model_info_noise',\t'model_info_group_size']\n",
    "model_vars_title =  [s.rsplit('_', 1)[-1] for s in model_vars]\n",
    "\n",
    "data['param_model'] = data.loc[:, model_vars].astype(str).agg('_'.join, axis=1)\n",
    "print('Shape of data', data.shape)\n",
    "data['param_model'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model_info_model\n",
      "rf     4800\n",
      "xgb    4800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "model_info_n_train\n",
      "200     4800\n",
      "1000    4800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "model_info_n_test\n",
      "100000    9600\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "model_info_n_features\n",
      "8    9600\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "model_info_noise\n",
      "0    4800\n",
      "3    4800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "model_info_transformation\n",
      "identity    3200\n",
      "log         3200\n",
      "sqrt        3200\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "model_info_group_size\n",
      "5     4800\n",
      "10    4800\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display unique values for each column with counts\n",
    "model_info = ['model_info_model', 'model_info_n_train', 'model_info_n_test',\n",
    "       'model_info_n_features', 'model_info_noise',\n",
    "       'model_info_transformation', 'model_info_group_size']\n",
    "for column in model_info:\n",
    "    unique_values = data[column].value_counts()\n",
    "    print(f\"\\n{unique_values}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slds_env310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
